\section{Hinderniserkennung}

Wie bereits im \acrshort{pren1} klar wurde, ist das Problem der Hinderniserkennung zu komplex,
um mit simpler Bildverarbeitung gelöst werden kann. 
Das Erkennen von Objekten auf einem Bild, wurde bereits durch \acrfull{cnn} gelöst.

\subsection{Objektdetektierung mit \acrshort{cnn}}

Die Objektdetektierung ist eines von vier Grundproblemen des {\it Computer Vision} Forschungsbereiches.
Dazu zählen die Bildklassifizierung, Objektdetektierung, Semantische Segmentation und Instance Segmentation.

\image{img/CNN/computer-vision-research-fields}
{
Der Forschungsbereich der {\it Computer Vision} \cite{wu2019recent} besteht aus Bildklassifizierung, 
Objektdetektierung, Semantische Segmentation und Instance Segmentation.
}

Bei der Objektdetektierung werden einzelne Objekte einer bestimmten Klasse mit {\it Bounding Boxes}
eingezeichnet. Die Genauigkeit solcher \acrshort{cnn} kann für ein spezialisiertes Gebiet die
eines Menschen übertreffen \cite{BUETTIDINH2019e00321}.

\subsection{Algorythmen zur Objektdetektierung}

Es gibt verschiedene Algorithmen zur detektierung von Objekten in einem Bild. 
Dabei ist vor allem die Abwägung zwischen Genauigkeit und Geschwindigkeit entscheidend.
Aus diesem Grund wird die Objektdetektierung auch in zwei Gruppen unterteilt, nähmlich 
single-stage detection wie \acrshort{yolo} \cite{redmon2016look} oder \acrshort{ssd} \cite{Liu_2016} und two-stage detection wie \acrfull{r-cnn} \cite{girshick2015fast}.

Ein two-stage detector besteht aus einem ersten Feature Proposal Schritt, gefolgt von einer Klassifizierung der 
vorgeschlagenen Features des ersten Schrittes. Ein sigle-stage detector klassifiziert die Regionen eines Bildes
direkt in einem Durchlauf. 

Während single-stage Detektoren generell eine schnellere Inference bieten, sind two-stage Detektoren genauer \cite{soviany2018optimizing}.
Aus diesem Grund werden häufig für Real-Time Objektdetektierung, wie das Erkennen von Objekten in einem Videostream, single-stage
Detektoren verwendet.

\subsection{\acrfull{r-cnn}}

Aufgrund der erhöhten Genauigkeit von two-stage Detektoren, wurde in dieser Arbeit vor allem \acrshort{r-cnn} angeschaut.

Bei einem \acrshort{r-cnn} \cite{girshick2014rich} werden zuerst interessante Regionen von einem \acrshort{cnn} für die 
spätere Segmantische Kategorisierung vorgeschlagen. Laut dem Paper von \acrshort{r-cnn}, wird eine um 30\% erhöhte Genauigkeit 
gegenüber vorherigen Objekterkennungsalgorythmen auf dem VOC 2012 Datenset erreicht. Damit wird eine \acrfull{map} von 53.3\%
auf dem VOC 2012 Datenset erreicht.

Bei einem \acrshort{r-cnn} 

\image{img/CNN/r-cnn}
{
  Der \acrfull{r-cnn} \cite{girshick2014rich} Objektdetektierungs Algorythmus bei welchem zuerst Regionen 
  vorgeschlagen werden, auf denen danach mit einem Backbone Segmantische Kategorisierung durchgeführt wird und
  das Objekt einer Klasse zugeteilt wird.
}

\subsection{Erkennung von Ziegelsteinen und Treppenstufen}

\subsubsection{Vereinfachtes \acrshort{r-cnn}}

Ein erster simpler versuch die Hindernisse (Ziegelsteine) zu erkennen war, mittels Feature Engineering\footnote{
In diesem Fall wurde Canny Edge detection \cite{canny-edge-detection} angewendet}, die Treppenstufen zu
erkennen. Ein Sliding Window auf der Treppenstufe dient dabei als Feature Proposal.
Die Kategorisierung wurde mit einem einfachen \acrshort{cnn} basierend auf den Sliding Window features 
gemacht.

\image{img/CNN/sliding-window-stair}
{
Ansatz eines vereinfachten \acrshort{r-cnn} mit Stufenerkennungs Feature engineering.
}

Die Resultate konnten weder die erwartete Performanz noch Genauigkeit zufriedenstellen.
Das Paper dazu kann auf GitHub \footnote{
https://github.com/randombenj/hslu-pren/blob/a987e1c53f3b44a86b42ca3030afb1f12880d94c/pren1/research/yes-no-classification-sliding-window.ipynb
} und im Anhang gefunden werden \ref{sec:r-cnn-paper}.


\subsubsection{Detectron2}

Detectron2 \cite{wu2019detectron2} ist ein State of the Art Framework für Objektdetektierung, Semantische- und Instance Segmentierung.
Es werden verschiedene \acrshort{r-cnn} Models zur verfügung gestellt. \footnote{
https://github.com/facebookresearch/detectron2/blob/master/MODEL\_ZOO.md\#coco-object-detection-baselines
}
Zudem bietet Detectron2 eine sehr einfache Möglichkeit mittels Transfer Learning \footnote{https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5\#scrollTo=b2bjrfb2LDeo}
einen eigenen Datensatz, bestehend auf dem vortrainierten Gewichten der Features zu trainieren.
Somit ist es möglich mit wenigen hundert Bildern neue Objekte zu erkennen.

\subsubsection{ONNX}

ONNX \footnote{https://github.com/onnx/onnx} ist ein Open Source Standard für Machine Learning
interoperability. Das heisst, ein Model welches mit Detectron2 (welches auf Pytorch basiert) trainiert
wurde, kann als ONNX format exportiert werden.
Dadurch, dass \acrshort{CNN} aus gewichteten Matrixoperationen bestehen, werden diese in das ONNX Format
exportiert und können danach mit einer Optimierten Inference Runtime ausgeführt werden.

\subsubsection{Performance Testing}

Um zu sehen ob ein Raspberry Pi solch komplexes \acrshort{CNN} überhaupt berechnen kann, oder ob ein
Leistungsfähigeres Compute Board gekauft werden muss, wurden Performance Tests durchgeführt.
Das setup für die Performance tests kann auf GitHub \footnote{https://github.com/randombenj/detectron2onnx-inference/blob/master/detectron2-onnx.ipynb} oder im Anhang
gefunden werden.

Für die Performance Tests wurden drei Controller angeschaut, die Controller konnten ausgeliehen werden.

\begin{itemize}
    \item Raspberry Pi
    \item Jetson Nano (4GB)
    \item Jetson TX1
\end{itemize}

\image{img/CNN/boards}{
Das Jetson TX1 (unten), Jetson Nano (oben mitte) und das Raspberry Pi (oben rechts und links)
}

Die Performance tests wurden mit dem oben genannten Notebook gemacht, zusätzlich wurde die Zeit für die
Inference gemessen. Die Resultate haben (ohne ONNX optimierung) eine Inference Zeit ergeben voné

\begin{itemize}
    \item {\bf Raspberry Pi}  ~500s
    \item {\bf Jetson Nano (4GB)} ~10s
    \item {\bf Jetson TX1} ~2s
\end{itemize}

Zwar konnte mit ONNX optimierung beim Raspberry Pi eine Inference Zeit von ~20s erreicht werden, 
dann sind aber alle vier CPU Kerne zu 100\% für 20s ausgelastet.

Da die Nvidia Boards jeweils über eine Maxwell GPU verfügen und diese auch mit CUDA \footnote{https://developer.nvidia.com/cuda-zone}
für Machine Learning verwendet werden kann, wurde entschieden, das Jetson Nano anstatt das ursprünglich geplante
Raspberry Pi zu kaufen. Das Jetson Nano verfügt über die gleichen Kamera und GPIO anschlüsse, und 
ist so Kompatibel mit der geplanten Elektronik.

\subsection{Trainieren des \acrshort{cnn}}

Bei den detectron2 \acrshort{cnn} Modellen kann bereits vor trainiertes Neuronales Netz
verwendet werden. Dieses kann danach mit wenigen hundert Bildern via Transfer Learning \footnote{https://en.wikipedia.org/wiki/Transfer\_learning}
auf ein eigenes Datenset angepasst werden.

Den vorgang kann man sich so vorstellen, dass das Modell verschiedene, häufig vorkommende Formen wie Kreise, Vierecke, ...
bereits kennt und man durch das Datenset dem Modell beibringt was eine Treppenstufe oder ein Ziegelstein ist.

\subsubsection{Sammeln von Trainingsdaten}

Die Trainingsdaten wurden mithilfe eines Kartonprototypen und einer kleinen App gesammelt.
Im Kartonprototypen wurde die Kamera auf der gleichen Höhe und im gleichen Winkel eingebaut
wie im endgerät.
Mithilfe der App auf dem gerät, konnten über das Smartphone einfach Bilder aufgenommen werden.

Die Bilder wurden in einer Auflösung von 720x960px aufgenommen. Ziel war es etwa 300 Bilder zu sammeln um für das \acrshort{CNN} zu annotieren.

\image{img/CNN/pren-image-capture}{
Aufnahme der Trainingsdaten für die Hinderniserkennung
}

\subsubsection{Annotieren der Trainingsdaten}

Die Trainingsdaten wurden nach dem Aufnehmen mithilfe von COCO Annotator \footnote{https://github.com/jsbroks/coco-annotator} annotiert.
Dabei wurden zwei Klassen verwendet: {\it Brick} und {\it Stair}.

\image{img/CNN/coco-stair}{
Annotieren der Trainingsdaten im COCO Datenformat \footnote{https://cocodataset.org/#format-data}.
}